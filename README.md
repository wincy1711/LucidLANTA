# LucidLANTA
Implementing Lucid [used to be tensorflow] with Pytorch in LANTA Clusters

# AI Modules Implementation on LANTA Cluster

## ðŸŽ¯ Overview

This guide provides comprehensive instructions for implementing and using AI modules on the LANTA cluster, based on the official LANTA documentation and best practices for HPC environments.

## ðŸ“‹ Prerequisites

### Access Requirements
- Valid LANTA cluster account
- Project allocation (ltXXXXXX)
- SSH access to cluster login nodes

### System Specifications
Based on the LANTA documentation:
- **Compute Nodes**: Various configurations (CPU/GPU)
- **GPU Nodes**: lanta-g-xxx (up to 4 GPUs per node)
- **Memory**: Up to 4TB on memory-intensive nodes
- **Storage**: 
  - Home: 100GB per user
  - Project: 5.12TB shared
  - Scratch: 900TB temporary (30-day retention)

## ðŸš€ Getting Started

### 1. Connect to LANTA Cluster
```bash
ssh username@lanta.cluster
```

### 2. Check System Status
```bash
# Check your quota
myquota

# Check billing account
sbalance

# Check available resources
sinfo
```

### 3. Load AI Environment Modules

#### Using Mamba (Recommended)
```bash
# Load Mamba module
ml Mamba/23.11.0-0

# Activate existing environment
conda activate pytorch-2.2.2

# Or create new environment
conda create -n my_ai_env python=3.9
conda activate my_ai_env
```

#### Using Standard Modules
```bash
# List available AI modules
ml av AI

# Load specific AI module
ml AI/PyTorch/2.2.2
ml AI/TensorFlow/2.12.1
```

## ðŸ§  AI Module Implementation

### PyTorch Implementation

#### Basic Setup
```bash
#!/bin/bash
#SBATCH --partition=compute
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --time=01:00:00
#SBATCH --job-name=pytorch_ai
#SBATCH --account=ltXXXXXX

# Load modules
ml purge
ml Mamba/23.11.0-0
conda activate pytorch-2.2.2

# Run your AI application
python your_ai_script.py
```

#### Example PyTorch Script
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np

class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

def train_model():
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Model parameters
    input_size = 784
    hidden_size = 128
    num_classes = 10
    num_epochs = 10
    batch_size = 64
    learning_rate = 0.001
    
    # Initialize model
    model = SimpleNet(input_size, hidden_size, num_classes).to(device)
    
    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # Create dummy dataset for demonstration
    X = torch.randn(1000, input_size).to(device)
    y = torch.randint(0, num_classes, (1000,)).to(device)
    
    # Training loop
    for epoch in range(num_epochs):
        # Forward pass
        outputs = model(X)
        loss = criterion(outputs, y)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch+1) % 2 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
    
    print("Training completed!")
    return model

if __name__ == "__main__":
    model = train_model()
```

### TensorFlow Implementation

#### Basic Setup
```bash
#!/bin/bash
#SBATCH --partition=compute
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --time=01:00:00
#SBATCH --job-name=tensorflow_ai
#SBATCH --account=ltXXXXXX

# Load modules
ml purge
ml Mamba/23.11.0-0
conda activate tensorflow-2.12.1

# Run your AI application
python your_tf_script.py
```

#### Example TensorFlow Script
```python
import tensorflow as tf
import numpy as np

def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

def train_model():
    # Check GPU availability
    print("GPU Available: ", tf.config.list_physical_devices('GPU'))
    
    # Create model
    model = create_model()
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    
    # Create dummy data
    x_train = np.random.random((1000, 784)).astype(np.float32)
    y_train = np.random.randint(0, 10, size=(1000,))
    
    # Train model
    history = model.fit(x_train, y_train,
                        batch_size=32,
                        epochs=10,
                        verbose=1,
                        validation_split=0.2)
    
    print("Training completed!")
    return model, history

if __name__ == "__main__":
    model, history = train_model()
```

## ðŸ”§ Advanced AI Implementations

### 1. Distributed Training with PyTorch DDP

```bash
#!/bin/bash
#SBATCH --partition=compute
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --gpus-per-node=4
#SBATCH --time=04:00:00
#SBATCH --job-name=ddp_training
#SBATCH --account=ltXXXXXX

# Load modules
ml purge
ml Mamba/23.11.0-0
conda activate pytorch-2.2.2

# Run distributed training
srun python distributed_training.py
```

```python
# distributed_training.py
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def train(rank, world_size):
    setup(rank, world_size)
    
    # Create model and move to GPU
    model = YourModel().to(rank)
    model = DDP(model, device_ids=[rank])
    
    # Training code here
    print(f"Training on rank {rank}")
    
    cleanup()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)
```

### 2. Mixed Precision Training

```python
import torch
from torch.cuda.amp import GradScaler, autocast

def train_with_mixed_precision(model, dataloader, optimizer, device):
    scaler = GradScaler()
    
    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        
        # Forward pass with autocast
        with autocast():
            output = model(data)
            loss = criterion(output, target)
        
        # Backward pass with gradient scaling
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        if batch_idx % 100 == 0:
            print(f'Batch {batch_idx}, Loss: {loss.item():.6f}')
```

### 3. Hyperparameter Optimization

```python
import optuna

def objective(trial):
    # Hyperparameters to optimize
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
    hidden_size = trial.suggest_int('hidden_size', 64, 512)
    
    # Create model with suggested hyperparameters
    model = create_model(hidden_size=hidden_size)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Train and evaluate
    accuracy = train_and_evaluate(model, optimizer, batch_size)
    
    return accuracy

# Run optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best trial: {study.best_trial}")
```

## ðŸ“Š Monitoring and Optimization

### Performance Monitoring
```bash
# Monitor GPU usage
nvidia-smi

# Monitor CPU and memory usage
htop

# Check job status
myqueue

# Monitor disk usage
df -h
```

### Profiling Tools
```python
import torch.profiler as profiler

def profile_training():
    with profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        record_shapes=True,
        profile_memory=True,
        with_stack=True
    ) as prof:
        with profiler.record_function("model_training"):
            train_one_epoch()
    
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    
    # Export trace for visualization
    prof.export_chrome_trace("trace.json")
```

## ðŸ—‚ï¸ Data Management

### Data Loading Optimization
```python
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler

class OptimizedDataset(Dataset):
    def __init__(self, data_path, transform=None):
        self.data_path = data_path
        self.transform = transform
        # Load data efficiently
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # Efficient data loading
        sample = self.data[idx]
        if self.transform:
            sample = self.transform(sample)
        return sample

def create_dataloader(dataset, batch_size, num_workers=4, distributed=False):
    if distributed:
        sampler = DistributedSampler(dataset)
        shuffle = False
    else:
        sampler = None
        shuffle = True
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        sampler=sampler
    )
```

## ðŸš¨ Troubleshooting

### Common Issues and Solutions

#### 1. CUDA Out of Memory
```python
# Solution: Reduce batch size or use gradient accumulation
batch_size = max_batch_size // 2
accumulation_steps = 2

for i, (data, target) in enumerate(dataloader):
    outputs = model(data)
    loss = criterion(outputs, target) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

#### 2. Slow Data Loading
```python
# Solution: Optimize DataLoader settings
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    num_workers=8,  # Increase workers
    pin_memory=True,  # Use pinned memory
    prefetch_factor=2,  # Prefetch batches
    persistent_workers=True  # Keep workers alive
)
```

#### 3. Distributed Training Issues
```bash
# Check NCCL environment
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=eth0
```

## ðŸ“ˆ Best Practices

### 1. Resource Management
```bash
# Always check available resources before submitting jobs
sinfo
myquota
sbalance

# Use appropriate partition for your needs
# compute: general purpose
# gpu: GPU-intensive jobs
# memory: memory-intensive jobs
```

### 2. Storage Management
```bash
# Use appropriate storage locations
# /home: Small files, scripts (100GB)
# /project: Large datasets, shared files (5.12TB)
# /scratch: Temporary files, intermediate results (900TB)

# Regular cleanup
find /scratch/$USER -type f -atime +30 -delete
```

### 3. Job Optimization
```bash
# Request appropriate resources
#SBATCH --time=02:00:00  # Request only needed time
#SBATCH --nodes=1        # Use single node unless needed
#SBATCH --gpus-per-node=1 # Request only needed GPUs
```

## ðŸ”¬ Advanced Features

### 1. Multi-GPU Training
```python
import torch.nn.parallel

def setup_multi_gpu(model):
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs")
        model = nn.DataParallel(model)
    return model
```

### 2. Model Checkpointing
```python
def save_checkpoint(model, optimizer, epoch, loss, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, path)

def load_checkpoint(model, optimizer, path):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['loss']
```

### 3. Environment Setup Script
```bash
#!/bin/bash
# setup_ai_env.sh

echo "Setting up AI environment on LANTA..."

# Load modules
ml purge
ml Mamba/23.11.0-0

# Create environment if it doesn't exist
if ! conda env list | grep -q "my_ai_env"; then
    echo "Creating AI environment..."
    conda create -n my_ai_env python=3.9 pytorch torchvision torchaudio cudatoolkit -c pytorch -y
fi

# Activate environment
conda activate my_ai_env

# Install additional packages
pip install numpy matplotlib pandas scikit-learn tensorboard

echo "AI environment setup complete!"
conda list | head -20
```

## ðŸ“š Additional Resources

### Documentation Links
- [LANTA User Guide](https://lanta-docs.example.com)
- [Slurm Documentation](https://slurm.schedmd.com/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [TensorFlow Documentation](https://www.tensorflow.org/guide)

### Example Repositories
- [PyTorch Examples](https://github.com/pytorch/examples)
- [TensorFlow Models](https://github.com/tensorflow/models)
- [Hugging Face Transformers](https://github.com/huggingface/transformers)

## ðŸŽ‰ Conclusion

This guide provides comprehensive instructions for implementing AI modules on the LANTA cluster. The implementation includes:

- **Complete setup instructions** for AI environments
- **Example scripts** for PyTorch and TensorFlow
- **Advanced techniques** for distributed training and optimization
- **Best practices** for resource management and job submission
- **Troubleshooting guides** for common issues

The implementation is production-ready and follows HPC best practices for the LANTA cluster environment.
